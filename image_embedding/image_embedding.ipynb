{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59b587e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1DW3BwSZEPl8JyArZPv5J6p3PQad7XiAy?usp=drive_link\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe55c13",
   "metadata": {},
   "source": [
    "## Install and import libraries\n",
    "\n",
    "Install and import the necessary libraries, including `transformers` and `torch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad46c4d",
   "metadata": {},
   "source": [
    "Import the installed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa87a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c369e9",
   "metadata": {},
   "source": [
    "## Load clip model and processor\n",
    "\n",
    "Download and load the CLIP model and its corresponding processor locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4281302",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = transformers.CLIPModel.from_pretrained(model_name)\n",
    "processor = transformers.CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d11586",
   "metadata": {},
   "source": [
    "## Load images\n",
    "\n",
    "Load the images you want to create embeddings for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1453f6",
   "metadata": {},
   "source": [
    "\n",
    "Create a list of image file paths and load the images using PIL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75dee5c",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "Use the provided KaggleHub code to download the dataset containing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52df645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Download the dataset and get the path\n",
    "dataset_path = kagglehub.dataset_download(\"gunhcolab/object-detection-dataset-standard-52card-deck\")\n",
    "\n",
    "# Assuming the images are in a directory named 'train' within the downloaded dataset\n",
    "image_dir = os.path.join(dataset_path, 'train') # Update with the correct image directory\n",
    "\n",
    "image_paths = []\n",
    "# Walk through the nested directories to find image files\n",
    "for root, _, files in os.walk(image_dir):\n",
    "    for f in files:\n",
    "        if f.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_paths.append(os.path.join(root, f))\n",
    "\n",
    "# Load all images\n",
    "images = []\n",
    "loaded_image_paths = [] # Keep track of paths for successfully loaded images\n",
    "for p in image_paths:\n",
    "    try:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        images.append(img)\n",
    "        loaded_image_paths.append(p) # Add path only if image is loaded successfully\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {p}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(images)} images.\")\n",
    "# Update image_paths to only include successfully loaded images\n",
    "image_paths = loaded_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9908d",
   "metadata": {},
   "source": [
    "Inspect the `kagglehub` module to find the correct function for getting the dataset path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6a695",
   "metadata": {},
   "source": [
    "## Create a search function\n",
    "\n",
    "Create a function to search the image embeddings using either text or image queries, processing them with the local CLIP model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0037ed",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Define a function that takes a query (text or image), processes it using the CLIP model and processor to get its embedding, calculates similarity with the image embeddings, and returns the indices of the most similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def search_images(query, image_features, model, processor, image_paths, top_k=5, batch_size=64):\n",
    "    \"\"\"\n",
    "    Searches for images based on a text or image query using CLIP embeddings with batch processing.\n",
    "\n",
    "    Args:\n",
    "        query: The search query (string for text or PIL Image for image).\n",
    "        image_features: The precomputed embeddings of the images.\n",
    "        model: The CLIP model.\n",
    "        processor: The CLIP processor.\n",
    "        image_paths: A list of paths to the images.\n",
    "        top_k: The number of top similar images to return.\n",
    "        batch_size: The batch size for calculating similarity.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples containing the similarity score and the path to the image.\n",
    "    \"\"\"\n",
    "    if isinstance(query, str):\n",
    "        # Process the text query\n",
    "        text_inputs = processor(text=query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            query_features = model.get_text_features(**text_inputs)\n",
    "    elif isinstance(query, Image.Image):\n",
    "        # Process the image query\n",
    "        image_inputs = processor(images=query, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            query_features = model.get_image_features(pixel_values=image_inputs.pixel_values)\n",
    "    else:\n",
    "        raise ValueError(\"Query must be a string (text) or a PIL Image.\")\n",
    "\n",
    "    # Normalize query features\n",
    "    query_features = F.normalize(query_features, p=2, dim=1)\n",
    "\n",
    "    similarity_scores = []\n",
    "    # Calculate similarity in batches\n",
    "    for i in range(0, image_features.shape[0], batch_size):\n",
    "        batch_image_features = image_features[i:i + batch_size]\n",
    "        # Normalize batch image features\n",
    "        batch_image_features = F.normalize(batch_image_features, p=2, dim=1)\n",
    "        batch_scores = (query_features @ batch_image_features.T).squeeze(0)\n",
    "        similarity_scores.append(batch_scores)\n",
    "\n",
    "    similarity_scores = torch.cat(similarity_scores, dim=0)\n",
    "\n",
    "\n",
    "    # Get the top k results\n",
    "    top_k_scores, top_k_indices = torch.topk(similarity_scores, top_k)\n",
    "\n",
    "    # Get the paths and scores of the top k images\n",
    "    results = [(top_k_scores[i].item(), image_paths[top_k_indices[i].item()]) for i in range(top_k)]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1fc680",
   "metadata": {},
   "source": [
    "## Create image embeddings\n",
    "\n",
    "\n",
    "Generate embeddings for the loaded images using the local CLIP model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c41ecb",
   "metadata": {},
   "source": [
    "\n",
    "Process the loaded images using the CLIP processor and generate embeddings using the CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for a subset of loaded images\n",
    "num_images_to_process = 100 # Set the number of images to process\n",
    "inputs = processor(images=images[:num_images_to_process], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = model.get_image_features(pixel_values=inputs.pixel_values)\n",
    "\n",
    "print(f\"Image embeddings created for {num_images_to_process} images.\")\n",
    "print(\"Shape of image embeddings:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0fbb3",
   "metadata": {},
   "source": [
    "Inspect the contents of the downloaded dataset directory to confirm the correct image directory and file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a06ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "\n",
    "# Get the path to the downloaded dataset\n",
    "dataset_path = kagglehub.dataset_download(\"gunhcolab/object-detection-dataset-standard-52card-deck\")\n",
    "\n",
    "# List the contents of the downloaded dataset directory\n",
    "print(f\"Contents of the dataset directory: {os.listdir(dataset_path)}\")\n",
    "\n",
    "# If there's a subdirectory for images, list its contents as well\n",
    "# Replace 'train' with the actual subdirectory name if different\n",
    "image_subdir = os.path.join(dataset_path, 'train')\n",
    "if os.path.exists(image_subdir):\n",
    "    print(f\"Contents of the image subdirectory ('train'): {os.listdir(image_subdir)[:20]}\") # Print only the first 20 items to avoid flooding the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "def display_projections(\n",
    "    labels: np.ndarray,\n",
    "    projections: np.ndarray,\n",
    "    image_paths: np.ndarray,\n",
    "    image_data_uris: Dict[str, str],\n",
    "    show_legend: bool = False,\n",
    "    show_markers_with_text: bool = True\n",
    ") -> None:\n",
    "    # Create a separate trace for each unique label\n",
    "    unique_labels = np.unique(labels)\n",
    "    traces = []\n",
    "    for unique_label in unique_labels:\n",
    "        mask = labels == unique_label\n",
    "        customdata_masked = image_paths[mask]\n",
    "        trace = go.Scatter3d(\n",
    "            x=projections[mask][:, 0],\n",
    "            y=projections[mask][:, 1],\n",
    "            z=projections[mask][:, 2],\n",
    "            mode='markers+text' if show_markers_with_text else 'markers',\n",
    "            text=labels[mask],\n",
    "            customdata=customdata_masked,\n",
    "            name=str(unique_label),\n",
    "            marker=dict(size=8),\n",
    "            hovertemplate=\"<b>class: %{text}</b><br>path: %{customdata}<extra></extra>\"\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    # Create the 3D scatter plot\n",
    "    fig = go.Figure(data=traces)\n",
    "    fig.update_layout(\n",
    "        scene=dict(xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        showlegend=show_legend\n",
    "    )\n",
    "\n",
    "    # Convert the chart to an HTML div string and add an ID to the div\n",
    "    plotly_div = fig.to_html(full_html=False, include_plotlyjs=False, div_id=\"scatter-plot-3d\")\n",
    "\n",
    "    # Define your JavaScript code for copying text on point click\n",
    "    javascript_code = f\"\"\"\n",
    "    <script>\n",
    "        function displayImage(imagePath) {{\n",
    "            var imageElement = document.getElementById('image-display');\n",
    "            var placeholderText = document.getElementById('placeholder-text');\n",
    "            var imageDataURIs = {image_data_uris};\n",
    "            imageElement.src = imageDataURIs[imagePath];\n",
    "            imageElement.style.display = 'block';\n",
    "            placeholderText.style.display = 'none';\n",
    "        }}\n",
    "\n",
    "        // Get the Plotly chart element by its ID\n",
    "        var chartElement = document.getElementById('scatter-plot-3d');\n",
    "\n",
    "        // Add a click event listener to the chart element\n",
    "        chartElement.on('plotly_click', function(data) {{\n",
    "            var customdata = data.points[0].customdata;\n",
    "            displayImage(customdata);\n",
    "        }});\n",
    "    </script>\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an HTML template including the chart div and JavaScript code\n",
    "    html_template = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "        <head>\n",
    "            <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n",
    "            <style>\n",
    "                #image-container {{\n",
    "                    position: fixed;\n",
    "                    top: 0;\n",
    "                    left: 0;\n",
    "                    width: 200px;\n",
    "                    height: 200px;\n",
    "                    padding: 5px;\n",
    "                    border: 1px solid #ccc;\n",
    "                    background-color: white;\n",
    "                    z-index: 1000;\n",
    "                    box-sizing: border-box;\n",
    "                    display: flex;\n",
    "                    align-items: center;\n",
    "                    justify-content: center;\n",
    "                    text-align: center;\n",
    "                }}\n",
    "                #image-display {{\n",
    "                    width: 100%;\n",
    "                    height: 100%;\n",
    "                    object-fit: contain;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            {plotly_div}\n",
    "            <div id=\"image-container\">\n",
    "                <img id=\"image-display\" src=\"\" alt=\"Selected image\" style=\"display: none;\" />\n",
    "                <p id=\"placeholder-text\">Click on a data entry to display an image</p>\n",
    "            </div>\n",
    "            {javascript_code}\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the HTML template in the Jupyter Notebook\n",
    "    display(HTML(html_template))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b90c03",
   "metadata": {},
   "source": [
    "## Prepare data for visualization\n",
    "\n",
    "Extract labels from image paths, reduce dimensionality of image embeddings, and create image data URIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ccb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# 1. Extract labels from image paths for the processed images\n",
    "# Assuming the label is the directory name immediately preceding the image file\n",
    "labels = np.array([os.path.basename(os.path.dirname(p)) for p in image_paths[:num_images_to_process]]) # Use only the processed image_paths\n",
    "\n",
    "# 2. Reduce dimensionality of image embeddings using PCA\n",
    "# Reduce to 3 components for 3D visualization.\n",
    "pca = PCA(n_components=3)\n",
    "projections = pca.fit_transform(image_features.cpu().numpy())\n",
    "\n",
    "# 3. Create image data URIs\n",
    "image_data_uris = {}\n",
    "# Create data URIs for all loaded images\n",
    "for i, img in enumerate(images[:num_images_to_process]): # Use only the processed images\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "    image_data_uris[image_paths[i]] = f\"data:image/png;base64,{img_str}\"\n",
    "\n",
    "print(\"Data prepared for visualization.\")\n",
    "print(\"Labels created:\", labels[:5])\n",
    "print(\"Projections shape:\", projections.shape)\n",
    "print(\"Image data URIs created for\", len(image_data_uris), \"images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda70c4",
   "metadata": {},
   "source": [
    "Now that `labels`, `projections`, and `image_data_uris` are defined, we can call the `display_projections` function to visualize the image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aff001",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_projections(\n",
    "    labels=labels,\n",
    "    projections=projections,\n",
    "    image_paths=np.array(image_paths[:num_images_to_process]), # Pass only the processed image_paths\n",
    "    image_data_uris=image_data_uris\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a7afe",
   "metadata": {},
   "source": [
    "## Perform search\n",
    "\n",
    "Use the search function to retrieve images based on a given text or image query.\\\n",
    "Call the `search_images` function with a sample text query and the generated image features to find similar images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ff0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a text-based search\n",
    "query_text = \"a playing card with a club\"\n",
    "search_results = search_images(query_text, image_features, model, processor, image_paths)\n",
    "\n",
    "print(f\"Search results for '{query_text}':\")\n",
    "for score, path in search_results:\n",
    "    print(f\"Similarity: {score:.4f}, Image Path: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ea564",
   "metadata": {},
   "source": [
    "## Display results\n",
    "\n",
    "Display the retrieved images based on the search results.\\\n",
    "Load and display the images from the paths returned by the search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311bf436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "print(\"Displaying top 5 search results:\")\n",
    "for score, path in search_results:\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        print(f\"Similarity: {score:.4f}\")\n",
    "        display(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying image {path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176d481",
   "metadata": {},
   "source": [
    "## Final task\n",
    "\n",
    "We have successfully built a pipeline to generate image embeddings using a locally downloaded CLIP model and retrieve images based on a text query. We installed the necessary libraries, loaded the CLIP model and processor, downloaded and loaded the images from a Kaggle dataset, created image embeddings, defined a search function, performed a text-based search, and displayed the top search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7072ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f5803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f2587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe17b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
